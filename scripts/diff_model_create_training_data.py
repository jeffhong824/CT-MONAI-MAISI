# Copyright (c) MONAI Consortium
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import argparse
import json
import logging
import os
from pathlib import Path

import monai
import nibabel as nib
import numpy as np
import torch
import torch.distributed as dist
from monai.transforms import Compose
from monai.utils import set_determinism
from monai.inferers.inferer import SlidingWindowInferer

from .diff_model_setting import initialize_distributed, load_config, setup_logging
from .transforms import define_fixed_intensity_transform, SUPPORT_MODALITIES
from .utils import define_instance, dynamic_infer


def create_transforms(dim: tuple = None, modality: str = 'unknown') -> Compose:
    """
    Create a set of MONAI transforms for preprocessing.

    Args:
        dim (tuple, optional): New dimensions for resizing. Defaults to None.

    Returns:
        Compose: Composed MONAI transforms.
    """
    if 'mri' in modality:
        modality = 'mri'
    if 'ct' in modality:
        modality = 'ct'
    
    if modality in SUPPORT_MODALITIES:        
        intensity_transforms = define_fixed_intensity_transform(modality=modality)        
    else:
        intensity_transforms = []
    
    if dim:
        return Compose(
            [
                monai.transforms.LoadImaged(keys="image"),
                monai.transforms.EnsureChannelFirstd(keys="image"),
                monai.transforms.Orientationd(keys="image", axcodes="RAS"),
                monai.transforms.EnsureTyped(keys="image", dtype=torch.float32)
            ]+intensity_transforms+[
                monai.transforms.Resized(keys="image", spatial_size=dim, mode="trilinear"),
            ]
        )
    else:
        return Compose(
            [
                monai.transforms.LoadImaged(keys="image"),
                monai.transforms.EnsureChannelFirstd(keys="image"),
                monai.transforms.Orientationd(keys="image", axcodes="RAS"),
            ] +intensity_transforms
        )


def round_number(number: int, base_number: int = 128) -> int:
    """
    Round the number to the nearest multiple of the base number, with a minimum value of the base number.

    Args:
        number (int): Number to be rounded.
        base_number (int): Number to be common divisor.

    Returns:
        int: Rounded number.
    """
    # Convert to float, divide by base, round to nearest integer, clamp to >= 1*base, and multiply back.
    new_number = max(round(float(number) / float(base_number)), 1.0) * float(base_number)
    return int(new_number)


# def load_filenames(data_list_path: str) -> list:
#     """
#     Load filenames from the JSON data list.

#     Args:
#         data_list_path (str): Path to the JSON data list file.

#     Returns:
#         list: List of filenames.
#     """
#     with open(data_list_path, "r") as file:
#         json_data = json.load(file)
#     # Expecting a MONAI-style list dict with "training": [{"image": "..."}]
#     filenames_raw = json_data["training"]
#     return [_item["image"] for _item in filenames_raw]


def process_file(
    filepath: str,
    args: argparse.Namespace,
    autoencoder: torch.nn.Module,
    device: torch.device,
    plain_transforms: Compose,
    new_transforms: Compose,
    logger: logging.Logger,
    modality: str = "ct",
    is_paired: bool = False,
    suffix: str = ""
) -> None:
    """
    Process a single file to create training data.

    Args:
        filepath (str): Path to the file to be processed (full path if is_paired, relative if not).
        args (argparse.Namespace): Configuration arguments.
        autoencoder (torch.nn.Module): Autoencoder model.
        device (torch.device): Device to process the file on.
        plain_transforms (Compose): Plain transforms.
        new_transforms (Compose): New transforms.
        logger (logging.Logger): Logger for logging information.
        modality (str): Modality type (e.g., "ct", "mri").
        is_paired (bool): Whether this is paired data (source/target).
        suffix (str): Suffix for output filename (e.g., "_src", "_tar").
    """
    # Determine full image path
    if is_paired:
        image_path = filepath  # Already full path
    else:
        image_path = os.path.join(args.data_base_dir, filepath)
    
    # Build output embedding filename
    # Extract base filename and create output path in embedding_base_dir
    base_filename = os.path.basename(filepath).replace(".nii.gz", "").replace(".nii", "")
    out_filename = os.path.join(args.embedding_base_dir, base_filename + suffix + "_emb.nii.gz")
    
    # Also save metadata JSON
    out_metadata_filename = out_filename + ".json"

    if os.path.isfile(out_filename) and os.path.isfile(out_metadata_filename):
        logger.info(f"Skipping {out_filename} (already exists)")
        return

    # Wrap input path into MONAI dict format.
    test_data = {"image": image_path}

    # Apply baseline transforms to read metadata like dim/spacing from original.
    transformed_data = plain_transforms(test_data)
    nda = transformed_data["image"]

    # Original volume size (dim) and spacing from nib/affine metadata.
    # Access meta BEFORE converting to numpy
    dim = [int(nda.meta["dim"][_i]) for _i in range(1, 4)]
    spacing = [float(nda.meta["pixdim"][_i]) for _i in range(1, 4)]

    logger.info(f"old dim: {dim}, old spacing: {spacing}")

    # Apply the full preprocessing (including resize if requested).
    new_data = new_transforms(test_data)
    nda_image = new_data["image"]

    # Keep the affine from the transformed image; convert to NumPy array for nibabel.
    # Access meta BEFORE converting to numpy
    new_affine = nda_image.meta["affine"].numpy()
    nda_image = nda_image.numpy().squeeze()  # [C, X, Y, Z] -> [X, Y, Z] since C=1

    logger.info(f"new dim: {nda_image.shape}, new affine: {new_affine}")

    try:
        # Ensure output directory exists.
        out_path = Path(out_filename)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        logger.info(f"out_filename: {out_filename}")

        # Mixed precision for encode pass (CUDA AMP); reduces memory/bandwidth.
        with torch.amp.autocast("cuda"):
            # Move preprocessed volume to device, add batch and channel dims -> [1,1,X,Y,Z]
            pt_nda = torch.from_numpy(nda_image).float().to(device).unsqueeze(0).unsqueeze(0)

            # Forward through autoencoder's stage-2 encoder to get latent z.
            inferer = SlidingWindowInferer(
                roi_size=[320, 320, 160],
                sw_batch_size=1,
                progress=True,
                mode="gaussian",
                overlap=0.4,
                sw_device=device,
                device=device,
            )
            z = dynamic_infer(inferer, autoencoder.encode_stage_2_inputs, pt_nda)
        
            # z = autoencoder.encode_stage_2_inputs(pt_nda)
            logger.info(f"z: {z.size()}, {z.dtype}")

            # Convert latent to NumPy, permute to [X,Y,Z,C], and save as NIfTI with the new affine.
            out_nda = z.squeeze().cpu().detach().numpy().transpose(1, 2, 3, 0)
            out_img = nib.Nifti1Image(np.float32(out_nda), affine=new_affine)
            nib.save(out_img, out_filename)
            
            # Save metadata JSON
            metadata = {
                "spacing": spacing,
                "modality": modality,
                "dim": dim,
                "new_dim": list(nda_image.shape),
            }
            with open(out_metadata_filename, "w") as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"Saved embedding: {out_filename}")
    except Exception as e:
        # Log and continue; do not crash the whole job on a single failure.
        logger.error(f"Error processing {filepath}: {e}")


@torch.inference_mode()
def diff_model_create_training_data(
    env_config_path: str, model_config_path: str, model_def_path: str, num_gpus: int
) -> None:
    """
    Create training data for the diffusion model.

    Args:
        env_config_path (str): Path to the environment configuration file.
        model_config_path (str): Path to the model configuration file.
        model_def_path (str): Path to the model definition file.
    """
    # Load merged config (env + model + definitions).
    args = load_config(env_config_path, model_config_path, model_def_path)

    # Initialize (potential) distributed environment; returns rank/world/device.
    local_rank, world_size, device = initialize_distributed(num_gpus=num_gpus)

    # Configure logger.
    logger = setup_logging("creating training data")
    logger.info(f"Using device {device}")

    # Instantiate autoencoder per config and load weights if available.
    autoencoder = define_instance(args, "autoencoder_def").to(device)
    checkpoint_autoencoder = torch.load(args.trained_autoencoder_path)
    if "unet_state_dict" in checkpoint_autoencoder.keys():
        checkpoint_autoencoder = checkpoint_autoencoder["unet_state_dict"]
    autoencoder.load_state_dict(checkpoint_autoencoder)

    # Ensure the embeddings output base directory exists.
    Path(args.embedding_base_dir).mkdir(parents=True, exist_ok=True)

    # Discover all training image file paths from JSON list.
    with open(args.json_data_list, "r") as file:
        json_data = json.load(file)
    
    files_raw = json_data["training"]
    
    # Check if this is paired data (has "source" and "target" keys)
    is_paired = len(files_raw) > 0 and "source" in files_raw[0] and "target" in files_raw[0]
    
    if is_paired:
        logger.info("Detected paired data format (source/target)")
    else:
        logger.info("Detected single image format")

    logger.info(f"Total files to process: {len(files_raw)}")

    # Baseline transforms (no resizing) to probe original metadata and orientation.
    plain_transforms = create_transforms(dim=None)

    # Static work partitioning over files: each rank processes files where idx % world_size == local_rank.
    for _iter in range(len(files_raw)):
        if _iter % world_size != local_rank:
            continue

        if is_paired:
            # Paired data: process both source and target
            source_filepath = files_raw[_iter]["source"]
            target_filepath = files_raw[_iter]["target"]
            modality = files_raw[_iter].get("modality", files_raw[_iter].get("class", "ct"))
            
            # Use source image to determine dimensions
            probe_data = {"image": source_filepath}
            transformed_probe = plain_transforms(probe_data)
            probe_nda = transformed_probe["image"]
            
            # Compute rounded target dims (multiples of 128) from the original image metadata.
            new_dim = tuple(
                round_number(int(probe_nda.meta["dim"][_i]))
                for _i in range(1, 4)
            )
            
            logger.info(f"Generate embeddings for paired data (modality: {modality})")
            new_transforms = create_transforms(new_dim, modality)
            
            # Process source image
            process_file(source_filepath, args, autoencoder, device, plain_transforms, new_transforms, logger, modality, is_paired=True, suffix="_src")
            
            # Process target image
            process_file(target_filepath, args, autoencoder, device, plain_transforms, new_transforms, logger, modality, is_paired=True, suffix="_tar")
        else:
            # Single image format (backward compatibility)
            filepath = files_raw[_iter].get("image", files_raw[_iter].get("path", ""))
            modality = files_raw[_iter].get("modality", files_raw[_iter].get("class", "ct"))
            
            # Compute rounded target dims (multiples of 128) from the original image metadata.
            probe_data = {"image": os.path.join(args.data_base_dir, filepath)}
            transformed_probe = plain_transforms(probe_data)
            probe_nda = transformed_probe["image"]
            
            new_dim = tuple(
                round_number(int(probe_nda.meta["dim"][_i]))
                for _i in range(1, 4)
            )
            
            logger.info(f"Generate embeddings assuming the data is {modality}")
            new_transforms = create_transforms(new_dim, modality)
            
            # Run the per-file preprocessing + autoencoder encoding + NIfTI saving.
            process_file(filepath, args, autoencoder, device, plain_transforms, new_transforms, logger, modality, is_paired=False)

    # Tear down distributed state if it was initialized.
    if dist.is_initialized():
        dist.destroy_process_group()


if __name__ == "__main__":
    # CLI entry: parse config paths and GPU count, then generate training data.
    parser = argparse.ArgumentParser(description="Diffusion Model Training Data Creation")
    parser.add_argument(
        "-e",
        "--env_config",
        type=str,
        default="./configs/environment_maisi_diff_model_train.json",
        help="Path to environment configuration file",
    )
    parser.add_argument(
        "-c",
        "--model_config",
        type=str,
        default="./configs/config_maisi_diff_model_train.json",
        help="Path to model training/inference configuration",
    )
    parser.add_argument(
        "-t",
        "--model_def", 
        type=str, 
        default="./configs/config_maisi.json", 
        help="Path to model definition file")
    parser.add_argument(
        "-g",
        "--num_gpus", 
        type=int, 
        default=1, 
        help="Number of GPUs to use for training"
    )

    args = parser.parse_args()
    diff_model_create_training_data(args.env_config, args.model_config, args.model_def, args.num_gpus)